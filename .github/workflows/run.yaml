# GitHub Actions Workflow for BOINCStats Scraper
# This workflow scrapes BOINCStats data daily and commits the results to the repository.

name: BOINCStats Scraper

on:
  # Daily at 02:00 UTC
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  scrape:
    name: Scrape BOINCStats
    runs-on: ubuntu-latest

    steps:
      # Checkout current repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      # Install Google Chrome
      - name: Install Google Chrome
        run: |
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      # Install Chromedriver
      - name: Install Chromedriver
        run: |
          sudo apt-get install -yqq unzip
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+' | head -1)
          CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
          wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
          unzip /tmp/chromedriver.zip -d /tmp
          sudo mv /tmp/chromedriver /usr/local/bin/chromedriver
          sudo chmod +x /usr/local/bin/chromedriver

      # Install Xvfb
      - name: Install Xvfb
        run: sudo apt-get install xvfb

      # Start Xvfb server
      - name: Start Xvfb
        run: |
          Xvfb :99 -screen 0 1920x1080x24 &
        env:
          DISPLAY: :99

      # Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Run scraper.py script
      - name: Run scraper
        env:
          DISPLAY: :99
        run: |
          python scraper.py

      # Check for DB files
      - name: Check DB files
        run: |
          ls -l
          cat db/* || echo "No DB files"

      # Configure Git for commit
      - name: Configure Git user
        run: |
          git config user.name "${{ secrets.USER }}"
          git config user.email "${{ secrets.EMAIL }}"

      # Commit updated BOINCStats data (only if there are changes)
      - name: Commit BOINCStats data
        run: |
          if [ -d db ] && [ "$(ls -A db)" ]; then
            git add db
            git commit -m "Automatic update of BOINCStats data [GitHub Actions]" || echo "No changes to commit"
            git push
          else
            echo "No db files to commit."
          fi

      # Push changes to the repo's master branch
      - name: Push changes
        env:
          GITHUB_TOKEN: ${{ secrets.PAT }}
        run: |
          git push https://x-access-token:${PAT}@github.com/${{ github.repository }}.git HEAD:master
